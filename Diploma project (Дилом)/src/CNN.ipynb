{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Batch [0/2962], Loss: 0.6982\n",
      "Epoch [1/15], Batch [100/2962], Loss: 0.6883\n",
      "Epoch [1/15], Batch [200/2962], Loss: 0.6836\n",
      "Epoch [1/15], Batch [300/2962], Loss: 0.5350\n",
      "Epoch [1/15], Batch [400/2962], Loss: 0.7061\n",
      "Epoch [1/15], Batch [500/2962], Loss: 0.7066\n",
      "Epoch [1/15], Batch [600/2962], Loss: 0.7033\n",
      "Epoch [1/15], Batch [700/2962], Loss: 0.6846\n",
      "Epoch [1/15], Batch [800/2962], Loss: 0.6861\n",
      "Epoch [1/15], Batch [900/2962], Loss: 0.5573\n",
      "Epoch [1/15], Batch [1000/2962], Loss: 0.7253\n",
      "Epoch [1/15], Batch [1100/2962], Loss: 0.5712\n",
      "Epoch [1/15], Batch [1200/2962], Loss: 0.6081\n",
      "Epoch [1/15], Batch [1300/2962], Loss: 0.6843\n",
      "Epoch [1/15], Batch [1400/2962], Loss: 0.6251\n",
      "Epoch [1/15], Batch [1500/2962], Loss: 0.5949\n",
      "Epoch [1/15], Batch [1600/2962], Loss: 0.6631\n",
      "Epoch [1/15], Batch [1700/2962], Loss: 0.6274\n",
      "Epoch [1/15], Batch [1800/2962], Loss: 0.6788\n",
      "Epoch [1/15], Batch [1900/2962], Loss: 0.6118\n",
      "Epoch [1/15], Batch [2000/2962], Loss: 0.6106\n",
      "Epoch [1/15], Batch [2100/2962], Loss: 0.5209\n",
      "Epoch [1/15], Batch [2200/2962], Loss: 0.5943\n",
      "Epoch [1/15], Batch [2300/2962], Loss: 0.6889\n",
      "Epoch [1/15], Batch [2400/2962], Loss: 0.6443\n",
      "Epoch [1/15], Batch [2500/2962], Loss: 0.6243\n",
      "Epoch [1/15], Batch [2600/2962], Loss: 0.5629\n",
      "Epoch [1/15], Batch [2700/2962], Loss: 0.5641\n",
      "Epoch [1/15], Batch [2800/2962], Loss: 0.5991\n",
      "Epoch [1/15], Batch [2900/2962], Loss: 0.5562\n",
      "Test Accuracy: 64.75%\n",
      "Epoch [2/15], Batch [0/2962], Loss: 0.5812\n",
      "Epoch [2/15], Batch [100/2962], Loss: 0.5247\n",
      "Epoch [2/15], Batch [200/2962], Loss: 0.7008\n",
      "Epoch [2/15], Batch [300/2962], Loss: 0.6518\n",
      "Epoch [2/15], Batch [400/2962], Loss: 0.5767\n",
      "Epoch [2/15], Batch [500/2962], Loss: 0.6034\n",
      "Epoch [2/15], Batch [600/2962], Loss: 0.5151\n",
      "Epoch [2/15], Batch [700/2962], Loss: 0.6773\n",
      "Epoch [2/15], Batch [800/2962], Loss: 0.8135\n",
      "Epoch [2/15], Batch [900/2962], Loss: 0.6334\n",
      "Epoch [2/15], Batch [1000/2962], Loss: 0.7056\n",
      "Epoch [2/15], Batch [1100/2962], Loss: 0.6642\n",
      "Epoch [2/15], Batch [1200/2962], Loss: 0.5956\n",
      "Epoch [2/15], Batch [1300/2962], Loss: 0.5813\n",
      "Epoch [2/15], Batch [1400/2962], Loss: 0.6085\n",
      "Epoch [2/15], Batch [1500/2962], Loss: 0.4886\n",
      "Epoch [2/15], Batch [1600/2962], Loss: 0.6387\n",
      "Epoch [2/15], Batch [1700/2962], Loss: 0.7408\n",
      "Epoch [2/15], Batch [1800/2962], Loss: 0.4939\n",
      "Epoch [2/15], Batch [1900/2962], Loss: 0.5763\n",
      "Epoch [2/15], Batch [2000/2962], Loss: 0.6586\n",
      "Epoch [2/15], Batch [2100/2962], Loss: 0.6539\n",
      "Epoch [2/15], Batch [2200/2962], Loss: 0.5154\n",
      "Epoch [2/15], Batch [2300/2962], Loss: 0.5337\n",
      "Epoch [2/15], Batch [2400/2962], Loss: 0.5802\n",
      "Epoch [2/15], Batch [2500/2962], Loss: 0.6832\n",
      "Epoch [2/15], Batch [2600/2962], Loss: 0.6847\n",
      "Epoch [2/15], Batch [2700/2962], Loss: 0.5661\n",
      "Epoch [2/15], Batch [2800/2962], Loss: 0.6194\n",
      "Epoch [2/15], Batch [2900/2962], Loss: 0.6734\n",
      "Test Accuracy: 69.33%\n",
      "Epoch [3/15], Batch [0/2962], Loss: 0.5756\n",
      "Epoch [3/15], Batch [100/2962], Loss: 0.4740\n",
      "Epoch [3/15], Batch [200/2962], Loss: 0.6712\n",
      "Epoch [3/15], Batch [300/2962], Loss: 0.5905\n",
      "Epoch [3/15], Batch [400/2962], Loss: 0.6833\n",
      "Epoch [3/15], Batch [500/2962], Loss: 0.5620\n",
      "Epoch [3/15], Batch [600/2962], Loss: 0.5992\n",
      "Epoch [3/15], Batch [700/2962], Loss: 0.5466\n",
      "Epoch [3/15], Batch [800/2962], Loss: 0.5846\n",
      "Epoch [3/15], Batch [900/2962], Loss: 0.5942\n",
      "Epoch [3/15], Batch [1000/2962], Loss: 0.5777\n",
      "Epoch [3/15], Batch [1100/2962], Loss: 0.4647\n",
      "Epoch [3/15], Batch [1200/2962], Loss: 0.7004\n",
      "Epoch [3/15], Batch [1300/2962], Loss: 0.5284\n",
      "Epoch [3/15], Batch [1400/2962], Loss: 0.7916\n",
      "Epoch [3/15], Batch [1500/2962], Loss: 0.5580\n",
      "Epoch [3/15], Batch [1600/2962], Loss: 0.6679\n",
      "Epoch [3/15], Batch [1700/2962], Loss: 0.5817\n",
      "Epoch [3/15], Batch [1800/2962], Loss: 0.5734\n",
      "Epoch [3/15], Batch [1900/2962], Loss: 0.5831\n",
      "Epoch [3/15], Batch [2000/2962], Loss: 0.5436\n",
      "Epoch [3/15], Batch [2100/2962], Loss: 0.5702\n",
      "Epoch [3/15], Batch [2200/2962], Loss: 0.5241\n",
      "Epoch [3/15], Batch [2300/2962], Loss: 0.5541\n",
      "Epoch [3/15], Batch [2400/2962], Loss: 0.7697\n",
      "Epoch [3/15], Batch [2500/2962], Loss: 0.5233\n",
      "Epoch [3/15], Batch [2600/2962], Loss: 0.6432\n",
      "Epoch [3/15], Batch [2700/2962], Loss: 0.7715\n",
      "Epoch [3/15], Batch [2800/2962], Loss: 0.5420\n",
      "Epoch [3/15], Batch [2900/2962], Loss: 0.5009\n",
      "Test Accuracy: 70.80%\n",
      "Epoch [4/15], Batch [0/2962], Loss: 0.6229\n",
      "Epoch [4/15], Batch [100/2962], Loss: 0.5307\n",
      "Epoch [4/15], Batch [200/2962], Loss: 0.4974\n",
      "Epoch [4/15], Batch [300/2962], Loss: 0.5711\n",
      "Epoch [4/15], Batch [400/2962], Loss: 0.5232\n",
      "Epoch [4/15], Batch [500/2962], Loss: 0.5028\n",
      "Epoch [4/15], Batch [600/2962], Loss: 0.5541\n",
      "Epoch [4/15], Batch [700/2962], Loss: 0.5883\n",
      "Epoch [4/15], Batch [800/2962], Loss: 0.7349\n",
      "Epoch [4/15], Batch [900/2962], Loss: 0.5388\n",
      "Epoch [4/15], Batch [1000/2962], Loss: 0.4569\n",
      "Epoch [4/15], Batch [1100/2962], Loss: 0.6544\n",
      "Epoch [4/15], Batch [1200/2962], Loss: 0.4813\n",
      "Epoch [4/15], Batch [1300/2962], Loss: 0.5660\n",
      "Epoch [4/15], Batch [1400/2962], Loss: 0.5475\n",
      "Epoch [4/15], Batch [1500/2962], Loss: 0.6432\n",
      "Epoch [4/15], Batch [1600/2962], Loss: 0.6328\n",
      "Epoch [4/15], Batch [1700/2962], Loss: 0.5707\n",
      "Epoch [4/15], Batch [1800/2962], Loss: 0.6035\n",
      "Epoch [4/15], Batch [1900/2962], Loss: 0.5460\n",
      "Epoch [4/15], Batch [2000/2962], Loss: 0.6541\n",
      "Epoch [4/15], Batch [2100/2962], Loss: 0.5433\n",
      "Epoch [4/15], Batch [2200/2962], Loss: 0.5578\n",
      "Epoch [4/15], Batch [2300/2962], Loss: 0.5795\n",
      "Epoch [4/15], Batch [2400/2962], Loss: 0.4699\n",
      "Epoch [4/15], Batch [2500/2962], Loss: 0.5939\n",
      "Epoch [4/15], Batch [2600/2962], Loss: 0.5097\n",
      "Epoch [4/15], Batch [2700/2962], Loss: 0.6611\n",
      "Epoch [4/15], Batch [2800/2962], Loss: 0.6265\n",
      "Epoch [4/15], Batch [2900/2962], Loss: 0.5426\n",
      "Test Accuracy: 70.86%\n",
      "Epoch [5/15], Batch [0/2962], Loss: 0.5070\n",
      "Epoch [5/15], Batch [100/2962], Loss: 0.4508\n",
      "Epoch [5/15], Batch [200/2962], Loss: 0.5508\n",
      "Epoch [5/15], Batch [300/2962], Loss: 0.5269\n",
      "Epoch [5/15], Batch [400/2962], Loss: 0.5844\n",
      "Epoch [5/15], Batch [500/2962], Loss: 0.6525\n",
      "Epoch [5/15], Batch [600/2962], Loss: 0.5749\n",
      "Epoch [5/15], Batch [700/2962], Loss: 0.5817\n",
      "Epoch [5/15], Batch [800/2962], Loss: 0.6084\n",
      "Epoch [5/15], Batch [900/2962], Loss: 0.5724\n",
      "Epoch [5/15], Batch [1000/2962], Loss: 0.5184\n",
      "Epoch [5/15], Batch [1100/2962], Loss: 0.5717\n",
      "Epoch [5/15], Batch [1200/2962], Loss: 0.5938\n",
      "Epoch [5/15], Batch [1300/2962], Loss: 0.4307\n",
      "Epoch [5/15], Batch [1400/2962], Loss: 0.4241\n",
      "Epoch [5/15], Batch [1500/2962], Loss: 0.5963\n",
      "Epoch [5/15], Batch [1600/2962], Loss: 0.5717\n",
      "Epoch [5/15], Batch [1700/2962], Loss: 0.6034\n",
      "Epoch [5/15], Batch [1800/2962], Loss: 0.5643\n",
      "Epoch [5/15], Batch [1900/2962], Loss: 0.6165\n",
      "Epoch [5/15], Batch [2000/2962], Loss: 0.5361\n",
      "Epoch [5/15], Batch [2100/2962], Loss: 0.5578\n",
      "Epoch [5/15], Batch [2200/2962], Loss: 0.4696\n",
      "Epoch [5/15], Batch [2300/2962], Loss: 0.6253\n",
      "Epoch [5/15], Batch [2400/2962], Loss: 0.5015\n",
      "Epoch [5/15], Batch [2500/2962], Loss: 0.6124\n",
      "Epoch [5/15], Batch [2600/2962], Loss: 0.4942\n",
      "Epoch [5/15], Batch [2700/2962], Loss: 0.6953\n",
      "Epoch [5/15], Batch [2800/2962], Loss: 0.5427\n",
      "Epoch [5/15], Batch [2900/2962], Loss: 0.5757\n",
      "Test Accuracy: 71.43%\n",
      "Epoch [6/15], Batch [0/2962], Loss: 0.5942\n",
      "Epoch [6/15], Batch [100/2962], Loss: 0.3916\n",
      "Epoch [6/15], Batch [200/2962], Loss: 0.6004\n",
      "Epoch [6/15], Batch [300/2962], Loss: 0.6548\n",
      "Epoch [6/15], Batch [400/2962], Loss: 0.5496\n",
      "Epoch [6/15], Batch [500/2962], Loss: 0.5745\n",
      "Epoch [6/15], Batch [600/2962], Loss: 0.6298\n",
      "Epoch [6/15], Batch [700/2962], Loss: 0.5511\n",
      "Epoch [6/15], Batch [800/2962], Loss: 0.6434\n",
      "Epoch [6/15], Batch [900/2962], Loss: 0.5480\n",
      "Epoch [6/15], Batch [1000/2962], Loss: 0.5970\n",
      "Epoch [6/15], Batch [1100/2962], Loss: 0.6250\n",
      "Epoch [6/15], Batch [1200/2962], Loss: 0.5047\n",
      "Epoch [6/15], Batch [1300/2962], Loss: 0.5592\n",
      "Epoch [6/15], Batch [1400/2962], Loss: 0.6568\n",
      "Epoch [6/15], Batch [1500/2962], Loss: 0.5232\n",
      "Epoch [6/15], Batch [1600/2962], Loss: 0.7091\n",
      "Epoch [6/15], Batch [1700/2962], Loss: 0.5356\n",
      "Epoch [6/15], Batch [1800/2962], Loss: 0.5998\n",
      "Epoch [6/15], Batch [1900/2962], Loss: 0.5071\n",
      "Epoch [6/15], Batch [2000/2962], Loss: 0.5801\n",
      "Epoch [6/15], Batch [2100/2962], Loss: 0.6198\n",
      "Epoch [6/15], Batch [2200/2962], Loss: 0.5740\n",
      "Epoch [6/15], Batch [2300/2962], Loss: 0.5648\n",
      "Epoch [6/15], Batch [2400/2962], Loss: 0.4413\n",
      "Epoch [6/15], Batch [2500/2962], Loss: 0.5462\n",
      "Epoch [6/15], Batch [2600/2962], Loss: 0.4919\n",
      "Epoch [6/15], Batch [2700/2962], Loss: 0.5632\n",
      "Epoch [6/15], Batch [2800/2962], Loss: 0.5749\n",
      "Epoch [6/15], Batch [2900/2962], Loss: 0.5594\n",
      "Test Accuracy: 72.35%\n",
      "Epoch [7/15], Batch [0/2962], Loss: 0.4659\n",
      "Epoch [7/15], Batch [100/2962], Loss: 0.5206\n",
      "Epoch [7/15], Batch [200/2962], Loss: 0.5487\n",
      "Epoch [7/15], Batch [300/2962], Loss: 0.6021\n",
      "Epoch [7/15], Batch [400/2962], Loss: 0.6125\n",
      "Epoch [7/15], Batch [500/2962], Loss: 0.6047\n",
      "Epoch [7/15], Batch [600/2962], Loss: 0.5853\n",
      "Epoch [7/15], Batch [700/2962], Loss: 0.5577\n",
      "Epoch [7/15], Batch [800/2962], Loss: 0.5208\n",
      "Epoch [7/15], Batch [900/2962], Loss: 0.4676\n",
      "Epoch [7/15], Batch [1000/2962], Loss: 0.5382\n",
      "Epoch [7/15], Batch [1100/2962], Loss: 0.5185\n",
      "Epoch [7/15], Batch [1200/2962], Loss: 0.4182\n",
      "Epoch [7/15], Batch [1300/2962], Loss: 0.5507\n",
      "Epoch [7/15], Batch [1400/2962], Loss: 0.5092\n",
      "Epoch [7/15], Batch [1500/2962], Loss: 0.5353\n",
      "Epoch [7/15], Batch [1600/2962], Loss: 0.5808\n",
      "Epoch [7/15], Batch [1700/2962], Loss: 0.4793\n",
      "Epoch [7/15], Batch [1800/2962], Loss: 0.5471\n",
      "Epoch [7/15], Batch [1900/2962], Loss: 0.4457\n",
      "Epoch [7/15], Batch [2000/2962], Loss: 0.4485\n",
      "Epoch [7/15], Batch [2100/2962], Loss: 0.6007\n",
      "Epoch [7/15], Batch [2200/2962], Loss: 0.4899\n",
      "Epoch [7/15], Batch [2300/2962], Loss: 0.5842\n",
      "Epoch [7/15], Batch [2400/2962], Loss: 0.6393\n",
      "Epoch [7/15], Batch [2500/2962], Loss: 0.5149\n",
      "Epoch [7/15], Batch [2600/2962], Loss: 0.4237\n",
      "Epoch [7/15], Batch [2700/2962], Loss: 0.5002\n",
      "Epoch [7/15], Batch [2800/2962], Loss: 0.5411\n",
      "Epoch [7/15], Batch [2900/2962], Loss: 0.5938\n",
      "Test Accuracy: 72.61%\n",
      "Epoch [8/15], Batch [0/2962], Loss: 0.5298\n",
      "Epoch [8/15], Batch [100/2962], Loss: 0.4366\n",
      "Epoch [8/15], Batch [200/2962], Loss: 0.6327\n",
      "Epoch [8/15], Batch [300/2962], Loss: 0.3231\n",
      "Epoch [8/15], Batch [400/2962], Loss: 0.5314\n",
      "Epoch [8/15], Batch [500/2962], Loss: 0.6281\n",
      "Epoch [8/15], Batch [600/2962], Loss: 0.5985\n",
      "Epoch [8/15], Batch [700/2962], Loss: 0.4449\n",
      "Epoch [8/15], Batch [800/2962], Loss: 0.4986\n",
      "Epoch [8/15], Batch [900/2962], Loss: 0.6921\n",
      "Epoch [8/15], Batch [1000/2962], Loss: 0.5083\n",
      "Epoch [8/15], Batch [1100/2962], Loss: 0.5321\n",
      "Epoch [8/15], Batch [1200/2962], Loss: 0.6304\n",
      "Epoch [8/15], Batch [1300/2962], Loss: 0.4374\n",
      "Epoch [8/15], Batch [1400/2962], Loss: 0.6001\n",
      "Epoch [8/15], Batch [1500/2962], Loss: 0.3856\n",
      "Epoch [8/15], Batch [1600/2962], Loss: 0.5736\n",
      "Epoch [8/15], Batch [1700/2962], Loss: 0.5198\n",
      "Epoch [8/15], Batch [1800/2962], Loss: 0.5066\n",
      "Epoch [8/15], Batch [1900/2962], Loss: 0.5514\n",
      "Epoch [8/15], Batch [2000/2962], Loss: 0.5588\n",
      "Epoch [8/15], Batch [2100/2962], Loss: 0.6302\n",
      "Epoch [8/15], Batch [2200/2962], Loss: 0.5852\n",
      "Epoch [8/15], Batch [2300/2962], Loss: 0.5434\n",
      "Epoch [8/15], Batch [2400/2962], Loss: 0.5150\n",
      "Epoch [8/15], Batch [2500/2962], Loss: 0.5667\n",
      "Epoch [8/15], Batch [2600/2962], Loss: 0.4880\n",
      "Epoch [8/15], Batch [2700/2962], Loss: 0.5471\n",
      "Epoch [8/15], Batch [2800/2962], Loss: 0.5570\n",
      "Epoch [8/15], Batch [2900/2962], Loss: 0.6820\n",
      "Test Accuracy: 72.20%\n",
      "Epoch [9/15], Batch [0/2962], Loss: 0.4030\n",
      "Epoch [9/15], Batch [100/2962], Loss: 0.5390\n",
      "Epoch [9/15], Batch [200/2962], Loss: 0.5585\n",
      "Epoch [9/15], Batch [300/2962], Loss: 0.4703\n",
      "Epoch [9/15], Batch [400/2962], Loss: 0.5117\n",
      "Epoch [9/15], Batch [500/2962], Loss: 0.4890\n",
      "Epoch [9/15], Batch [600/2962], Loss: 0.6435\n",
      "Epoch [9/15], Batch [700/2962], Loss: 0.3968\n",
      "Epoch [9/15], Batch [800/2962], Loss: 0.3485\n",
      "Epoch [9/15], Batch [900/2962], Loss: 0.5452\n",
      "Epoch [9/15], Batch [1000/2962], Loss: 0.5432\n",
      "Epoch [9/15], Batch [1100/2962], Loss: 0.5067\n",
      "Epoch [9/15], Batch [1200/2962], Loss: 0.6072\n",
      "Epoch [9/15], Batch [1300/2962], Loss: 0.6766\n",
      "Epoch [9/15], Batch [1400/2962], Loss: 0.6021\n",
      "Epoch [9/15], Batch [1500/2962], Loss: 0.6169\n",
      "Epoch [9/15], Batch [1600/2962], Loss: 0.4927\n",
      "Epoch [9/15], Batch [1700/2962], Loss: 0.4956\n",
      "Epoch [9/15], Batch [1800/2962], Loss: 0.4528\n",
      "Epoch [9/15], Batch [1900/2962], Loss: 0.4384\n",
      "Epoch [9/15], Batch [2000/2962], Loss: 0.5641\n",
      "Epoch [9/15], Batch [2100/2962], Loss: 0.4701\n",
      "Epoch [9/15], Batch [2200/2962], Loss: 0.5622\n",
      "Epoch [9/15], Batch [2300/2962], Loss: 0.6256\n",
      "Epoch [9/15], Batch [2400/2962], Loss: 0.5653\n",
      "Epoch [9/15], Batch [2500/2962], Loss: 0.6539\n",
      "Epoch [9/15], Batch [2600/2962], Loss: 0.4804\n",
      "Epoch [9/15], Batch [2700/2962], Loss: 0.4262\n",
      "Epoch [9/15], Batch [2800/2962], Loss: 0.5754\n",
      "Epoch [9/15], Batch [2900/2962], Loss: 0.7296\n",
      "Test Accuracy: 72.90%\n",
      "Epoch [10/15], Batch [0/2962], Loss: 0.4163\n",
      "Epoch [10/15], Batch [100/2962], Loss: 0.6144\n",
      "Epoch [10/15], Batch [200/2962], Loss: 0.4266\n",
      "Epoch [10/15], Batch [300/2962], Loss: 0.4628\n",
      "Epoch [10/15], Batch [400/2962], Loss: 0.5337\n",
      "Epoch [10/15], Batch [500/2962], Loss: 0.5466\n",
      "Epoch [10/15], Batch [600/2962], Loss: 0.4539\n",
      "Epoch [10/15], Batch [700/2962], Loss: 0.4507\n",
      "Epoch [10/15], Batch [800/2962], Loss: 0.4746\n",
      "Epoch [10/15], Batch [900/2962], Loss: 0.4162\n",
      "Epoch [10/15], Batch [1000/2962], Loss: 0.3946\n",
      "Epoch [10/15], Batch [1100/2962], Loss: 0.5285\n",
      "Epoch [10/15], Batch [1200/2962], Loss: 0.4520\n",
      "Epoch [10/15], Batch [1300/2962], Loss: 0.5330\n",
      "Epoch [10/15], Batch [1400/2962], Loss: 0.7055\n",
      "Epoch [10/15], Batch [1500/2962], Loss: 0.4501\n",
      "Epoch [10/15], Batch [1600/2962], Loss: 0.5352\n",
      "Epoch [10/15], Batch [1700/2962], Loss: 0.5047\n",
      "Epoch [10/15], Batch [1800/2962], Loss: 0.4691\n",
      "Epoch [10/15], Batch [1900/2962], Loss: 0.5883\n",
      "Epoch [10/15], Batch [2000/2962], Loss: 0.6606\n",
      "Epoch [10/15], Batch [2100/2962], Loss: 0.4438\n",
      "Epoch [10/15], Batch [2200/2962], Loss: 0.5436\n",
      "Epoch [10/15], Batch [2300/2962], Loss: 0.5612\n",
      "Epoch [10/15], Batch [2400/2962], Loss: 0.4986\n",
      "Epoch [10/15], Batch [2500/2962], Loss: 0.5971\n",
      "Epoch [10/15], Batch [2600/2962], Loss: 0.5062\n",
      "Epoch [10/15], Batch [2700/2962], Loss: 0.6617\n",
      "Epoch [10/15], Batch [2800/2962], Loss: 0.5992\n",
      "Epoch [10/15], Batch [2900/2962], Loss: 0.4798\n",
      "Test Accuracy: 73.29%\n",
      "Epoch [11/15], Batch [0/2962], Loss: 0.5317\n",
      "Epoch [11/15], Batch [100/2962], Loss: 0.3838\n",
      "Epoch [11/15], Batch [200/2962], Loss: 0.4857\n",
      "Epoch [11/15], Batch [300/2962], Loss: 0.5703\n",
      "Epoch [11/15], Batch [400/2962], Loss: 0.5450\n",
      "Epoch [11/15], Batch [500/2962], Loss: 0.6182\n",
      "Epoch [11/15], Batch [600/2962], Loss: 0.6225\n",
      "Epoch [11/15], Batch [700/2962], Loss: 0.4526\n",
      "Epoch [11/15], Batch [800/2962], Loss: 0.5687\n",
      "Epoch [11/15], Batch [900/2962], Loss: 0.5089\n",
      "Epoch [11/15], Batch [1000/2962], Loss: 0.6322\n",
      "Epoch [11/15], Batch [1100/2962], Loss: 0.4459\n",
      "Epoch [11/15], Batch [1200/2962], Loss: 0.5369\n",
      "Epoch [11/15], Batch [1300/2962], Loss: 0.5145\n",
      "Epoch [11/15], Batch [1400/2962], Loss: 0.6417\n",
      "Epoch [11/15], Batch [1500/2962], Loss: 0.7073\n",
      "Epoch [11/15], Batch [1600/2962], Loss: 0.5235\n",
      "Epoch [11/15], Batch [1700/2962], Loss: 0.5616\n",
      "Epoch [11/15], Batch [1800/2962], Loss: 0.7333\n",
      "Epoch [11/15], Batch [1900/2962], Loss: 0.4090\n",
      "Epoch [11/15], Batch [2000/2962], Loss: 0.7370\n",
      "Epoch [11/15], Batch [2100/2962], Loss: 0.4557\n",
      "Epoch [11/15], Batch [2200/2962], Loss: 0.5114\n",
      "Epoch [11/15], Batch [2300/2962], Loss: 0.5128\n",
      "Epoch [11/15], Batch [2400/2962], Loss: 0.4415\n",
      "Epoch [11/15], Batch [2500/2962], Loss: 0.4907\n",
      "Epoch [11/15], Batch [2600/2962], Loss: 0.5106\n",
      "Epoch [11/15], Batch [2700/2962], Loss: 0.4410\n",
      "Epoch [11/15], Batch [2800/2962], Loss: 0.6207\n",
      "Epoch [11/15], Batch [2900/2962], Loss: 0.4490\n",
      "Test Accuracy: 73.99%\n",
      "Epoch [12/15], Batch [0/2962], Loss: 0.4754\n",
      "Epoch [12/15], Batch [100/2962], Loss: 0.5643\n",
      "Epoch [12/15], Batch [200/2962], Loss: 0.5288\n",
      "Epoch [12/15], Batch [300/2962], Loss: 0.5083\n",
      "Epoch [12/15], Batch [400/2962], Loss: 0.5663\n",
      "Epoch [12/15], Batch [500/2962], Loss: 0.5464\n",
      "Epoch [12/15], Batch [600/2962], Loss: 0.5740\n",
      "Epoch [12/15], Batch [700/2962], Loss: 0.6014\n",
      "Epoch [12/15], Batch [800/2962], Loss: 0.5167\n",
      "Epoch [12/15], Batch [900/2962], Loss: 0.6506\n",
      "Epoch [12/15], Batch [1000/2962], Loss: 0.3840\n",
      "Epoch [12/15], Batch [1100/2962], Loss: 0.4086\n",
      "Epoch [12/15], Batch [1200/2962], Loss: 0.6398\n",
      "Epoch [12/15], Batch [1300/2962], Loss: 0.5325\n",
      "Epoch [12/15], Batch [1400/2962], Loss: 0.3524\n",
      "Epoch [12/15], Batch [1500/2962], Loss: 0.6923\n",
      "Epoch [12/15], Batch [1600/2962], Loss: 0.4806\n",
      "Epoch [12/15], Batch [1700/2962], Loss: 0.3407\n",
      "Epoch [12/15], Batch [1800/2962], Loss: 0.6548\n",
      "Epoch [12/15], Batch [1900/2962], Loss: 0.7818\n",
      "Epoch [12/15], Batch [2000/2962], Loss: 0.6461\n",
      "Epoch [12/15], Batch [2100/2962], Loss: 0.5623\n",
      "Epoch [12/15], Batch [2200/2962], Loss: 0.5290\n",
      "Epoch [12/15], Batch [2300/2962], Loss: 0.4849\n",
      "Epoch [12/15], Batch [2400/2962], Loss: 0.4316\n",
      "Epoch [12/15], Batch [2500/2962], Loss: 0.5539\n",
      "Epoch [12/15], Batch [2600/2962], Loss: 0.4512\n",
      "Epoch [12/15], Batch [2700/2962], Loss: 0.5378\n",
      "Epoch [12/15], Batch [2800/2962], Loss: 0.4706\n",
      "Epoch [12/15], Batch [2900/2962], Loss: 0.4913\n",
      "Test Accuracy: 71.79%\n",
      "Epoch [13/15], Batch [0/2962], Loss: 0.5040\n",
      "Epoch [13/15], Batch [100/2962], Loss: 0.3943\n",
      "Epoch [13/15], Batch [200/2962], Loss: 0.5080\n",
      "Epoch [13/15], Batch [300/2962], Loss: 0.5540\n",
      "Epoch [13/15], Batch [400/2962], Loss: 0.5710\n",
      "Epoch [13/15], Batch [500/2962], Loss: 0.6924\n",
      "Epoch [13/15], Batch [600/2962], Loss: 0.5262\n",
      "Epoch [13/15], Batch [700/2962], Loss: 0.4540\n",
      "Epoch [13/15], Batch [800/2962], Loss: 0.5484\n",
      "Epoch [13/15], Batch [900/2962], Loss: 0.5490\n",
      "Epoch [13/15], Batch [1000/2962], Loss: 0.6436\n",
      "Epoch [13/15], Batch [1100/2962], Loss: 0.5019\n",
      "Epoch [13/15], Batch [1200/2962], Loss: 0.5634\n",
      "Epoch [13/15], Batch [1300/2962], Loss: 0.4056\n",
      "Epoch [13/15], Batch [1400/2962], Loss: 0.3824\n",
      "Epoch [13/15], Batch [1500/2962], Loss: 0.7021\n",
      "Epoch [13/15], Batch [1600/2962], Loss: 0.6055\n",
      "Epoch [13/15], Batch [1700/2962], Loss: 0.5689\n",
      "Epoch [13/15], Batch [1800/2962], Loss: 0.5569\n",
      "Epoch [13/15], Batch [1900/2962], Loss: 0.5401\n",
      "Epoch [13/15], Batch [2000/2962], Loss: 0.5666\n",
      "Epoch [13/15], Batch [2100/2962], Loss: 0.4670\n",
      "Epoch [13/15], Batch [2200/2962], Loss: 0.4384\n",
      "Epoch [13/15], Batch [2300/2962], Loss: 0.5588\n",
      "Epoch [13/15], Batch [2400/2962], Loss: 0.3607\n",
      "Epoch [13/15], Batch [2500/2962], Loss: 0.4507\n",
      "Epoch [13/15], Batch [2600/2962], Loss: 0.5255\n",
      "Epoch [13/15], Batch [2700/2962], Loss: 0.5593\n",
      "Epoch [13/15], Batch [2800/2962], Loss: 0.5847\n",
      "Epoch [13/15], Batch [2900/2962], Loss: 0.3701\n",
      "Test Accuracy: 73.25%\n",
      "Epoch [14/15], Batch [0/2962], Loss: 0.5324\n",
      "Epoch [14/15], Batch [100/2962], Loss: 0.5838\n",
      "Epoch [14/15], Batch [200/2962], Loss: 0.7094\n",
      "Epoch [14/15], Batch [300/2962], Loss: 0.5209\n",
      "Epoch [14/15], Batch [400/2962], Loss: 0.6259\n",
      "Epoch [14/15], Batch [500/2962], Loss: 0.6487\n",
      "Epoch [14/15], Batch [600/2962], Loss: 0.6452\n",
      "Epoch [14/15], Batch [700/2962], Loss: 0.5242\n",
      "Epoch [14/15], Batch [800/2962], Loss: 0.6884\n",
      "Epoch [14/15], Batch [900/2962], Loss: 0.5263\n",
      "Epoch [14/15], Batch [1000/2962], Loss: 0.5297\n",
      "Epoch [14/15], Batch [1100/2962], Loss: 0.4377\n",
      "Epoch [14/15], Batch [1200/2962], Loss: 0.5470\n",
      "Epoch [14/15], Batch [1300/2962], Loss: 0.4665\n",
      "Epoch [14/15], Batch [1400/2962], Loss: 0.6061\n",
      "Epoch [14/15], Batch [1500/2962], Loss: 0.4396\n",
      "Epoch [14/15], Batch [1600/2962], Loss: 0.5755\n",
      "Epoch [14/15], Batch [1700/2962], Loss: 0.4935\n",
      "Epoch [14/15], Batch [1800/2962], Loss: 0.4517\n",
      "Epoch [14/15], Batch [1900/2962], Loss: 0.5143\n",
      "Epoch [14/15], Batch [2000/2962], Loss: 0.7946\n",
      "Epoch [14/15], Batch [2100/2962], Loss: 0.4281\n",
      "Epoch [14/15], Batch [2200/2962], Loss: 0.5505\n",
      "Epoch [14/15], Batch [2300/2962], Loss: 0.4729\n",
      "Epoch [14/15], Batch [2400/2962], Loss: 0.6565\n",
      "Epoch [14/15], Batch [2500/2962], Loss: 0.5379\n",
      "Epoch [14/15], Batch [2600/2962], Loss: 0.5237\n",
      "Epoch [14/15], Batch [2700/2962], Loss: 0.4326\n",
      "Epoch [14/15], Batch [2800/2962], Loss: 0.5337\n",
      "Epoch [14/15], Batch [2900/2962], Loss: 0.6095\n",
      "Test Accuracy: 73.03%\n",
      "Epoch [15/15], Batch [0/2962], Loss: 0.4414\n",
      "Epoch [15/15], Batch [100/2962], Loss: 0.3920\n",
      "Epoch [15/15], Batch [200/2962], Loss: 0.5070\n",
      "Epoch [15/15], Batch [300/2962], Loss: 0.6119\n",
      "Epoch [15/15], Batch [400/2962], Loss: 0.4495\n",
      "Epoch [15/15], Batch [500/2962], Loss: 0.4417\n",
      "Epoch [15/15], Batch [600/2962], Loss: 0.4861\n",
      "Epoch [15/15], Batch [700/2962], Loss: 0.4494\n",
      "Epoch [15/15], Batch [800/2962], Loss: 0.6165\n",
      "Epoch [15/15], Batch [900/2962], Loss: 0.5583\n",
      "Epoch [15/15], Batch [1000/2962], Loss: 0.6629\n",
      "Epoch [15/15], Batch [1100/2962], Loss: 0.6099\n",
      "Epoch [15/15], Batch [1200/2962], Loss: 0.4426\n",
      "Epoch [15/15], Batch [1300/2962], Loss: 0.4573\n",
      "Epoch [15/15], Batch [1400/2962], Loss: 0.3946\n",
      "Epoch [15/15], Batch [1500/2962], Loss: 0.4613\n",
      "Epoch [15/15], Batch [1600/2962], Loss: 0.4397\n",
      "Epoch [15/15], Batch [1700/2962], Loss: 0.3658\n",
      "Epoch [15/15], Batch [1800/2962], Loss: 0.5702\n",
      "Epoch [15/15], Batch [1900/2962], Loss: 0.4923\n",
      "Epoch [15/15], Batch [2000/2962], Loss: 0.4219\n",
      "Epoch [15/15], Batch [2100/2962], Loss: 0.4132\n",
      "Epoch [15/15], Batch [2200/2962], Loss: 0.5150\n",
      "Epoch [15/15], Batch [2300/2962], Loss: 0.6041\n",
      "Epoch [15/15], Batch [2400/2962], Loss: 0.4886\n",
      "Epoch [15/15], Batch [2500/2962], Loss: 0.3993\n",
      "Epoch [15/15], Batch [2600/2962], Loss: 0.4961\n",
      "Epoch [15/15], Batch [2700/2962], Loss: 0.4511\n",
      "Epoch [15/15], Batch [2800/2962], Loss: 0.6098\n",
      "Epoch [15/15], Batch [2900/2962], Loss: 0.5484\n",
      "Test Accuracy: 73.68%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "# Define the dataset and data loader\n",
    "train_folder = 'D:/Data/Train'\n",
    "test_folder = 'D:/Data/Test'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "train_dataset = datasets.ImageFolder(train_folder, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_folder, transform=transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Instantiate the neural network and define the loss function and optimizer\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the neural network\n",
    "for epoch in range(15):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print training progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{15}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Evaluate the neural network on the test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, targets in test_loader:\n",
    "            scores = model(data)\n",
    "            _, predicted = torch.max(scores.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель создана\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'person_detection_model_v3_74.pth')\n",
    "print('Модель создана')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
