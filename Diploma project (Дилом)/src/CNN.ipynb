{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "# Define the dataset and data loader\n",
    "train_folder = 'D:/Data/Train'\n",
    "test_folder = 'D:/Data/Test'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((480,640)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "train_dataset = datasets.ImageFolder(train_folder, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_folder, transform=transforms.Compose([\n",
    "    transforms.Resize((480, 640)),\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=9600, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=9600, shuffle=False)\n",
    "\n",
    "# Instantiate the neural network and define the loss function and optimizer\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the neural network\n",
    "for epoch in range(15):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print training progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{15}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Evaluate the neural network on the test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, targets in test_loader:\n",
    "            scores = model(data)\n",
    "            _, predicted = torch.max(scores.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'person_detection_model_v3_74.pth')\n",
    "print('Модель создана')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Определение архитектуры нейронной сети\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Загрузка весов модели\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('D:/Курсовой проект/Diploma project (Дилом)/src/CNN model/person_detection_model_v3_74.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Создание примерного входного тензора\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# Конвертация модели в формат ONNX\n",
    "onnx_path = 'person_detection_model.onnx'\n",
    "torch.onnx.export(model, dummy_input, onnx_path, opset_version=11)\n",
    "\n",
    "print('Модель успешно сконвертирована в формат ONNX и сохранена в файле person_detection_model.onnx.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx as onnx\n",
    "\n",
    "# Шаг 1: Загрузка модели PyTorch\n",
    "model = torch.load(\"D:/Курсовой проект/Diploma project (Дилом)/src/CNN model/person_detection_model_v3_74.pth\")  # Путь к вашей модели PyTorch\n",
    "\n",
    "# Шаг 2: Создание объекта ONNX и настройка его на использование CPU или GPU\n",
    "# Определение примерного входного размера (batch_size, channels, height, width)\n",
    "input_size = (1, 3, 32, 32)\n",
    "\n",
    "# Шаг 3: Преобразование модели PyTorch в ONNX\n",
    "dummy_input = torch.randn(*input_size)  # Создание примерного входного тензора\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]\n",
    "onnx_path = \"model.onnx\"  # Путь для сохранения модели ONNX\n",
    "\n",
    "# Шаг 4: Сохранение модели ONNX на диск\n",
    "torch.onnx.export(model, dummy_input, onnx_path, input_names=input_names, output_names=output_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.onnx\n",
    "\n",
    "# Загрузка предобученной модели Faster R-CNN с ResNet50-FPN\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Заморозка параметров всех слоев\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Замена классификатора модели на классификатор, специфичный для распознавания только людей\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "num_classes = 2  # 1 класс для фона и 1 класс для людей\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Создание примера входных данных\n",
    "input_data = torch.randn(1, 3, 480, 640)  # Пример входных данных размером 1x3x224x224\n",
    "\n",
    "# Установка имен входных и выходных тензоров\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]\n",
    "\n",
    "# Экспорт модели в формат ONNX\n",
    "torch.onnx.export(model,\n",
    "                  input_data,\n",
    "                  \"model.onnx\",\n",
    "                  input_names=input_names,\n",
    "                  output_names=output_names,\n",
    "                  opset_version=11)\n",
    "\n",
    "# Сохранение модели ONNX на диск\n",
    "torch.onnx.save(model, \"model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Загрузка модели Faster R-CNN из файла ONNX\n",
    "model = onnx.load(\"D:/Курсовой проект/Diploma project (Дилом)/src/OPNX_FPGA/model.onnx\")\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model)\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Загрузите предварительно обученную модель Faster R-CNN\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Экспортируйте модель в ONNX формат\n",
    "torch.onnx.export(model, torch.randn(1, 3, 224, 224), 'fasterrcnn_resnet50_fpn.onnx', opset_version=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\graphviz\\backend\\execute.py:81\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         proc \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mrun(cmd, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     82\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    546\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mstderr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m PIPE\n\u001b[1;32m--> 548\u001b[0m \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39;49mpopenargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m process:\n\u001b[0;32m    549\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\subprocess.py:1024\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1021\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[0;32m   1022\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m-> 1024\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m   1025\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m   1026\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m   1027\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m   1028\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m   1029\u001b[0m                         errread, errwrite,\n\u001b[0;32m   1030\u001b[0m                         restore_signals,\n\u001b[0;32m   1031\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m   1032\u001b[0m                         start_new_session, process_group)\n\u001b[0;32m   1033\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m   1034\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\subprocess.py:1509\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1508\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1509\u001b[0m     hp, ht, pid, tid \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mCreateProcess(executable, args,\n\u001b[0;32m   1510\u001b[0m                              \u001b[39m# no special security\u001b[39;49;00m\n\u001b[0;32m   1511\u001b[0m                              \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1512\u001b[0m                              \u001b[39mint\u001b[39;49m(\u001b[39mnot\u001b[39;49;00m close_fds),\n\u001b[0;32m   1513\u001b[0m                              creationflags,\n\u001b[0;32m   1514\u001b[0m                              env,\n\u001b[0;32m   1515\u001b[0m                              cwd,\n\u001b[0;32m   1516\u001b[0m                              startupinfo)\n\u001b[0;32m   1517\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1518\u001b[0m     \u001b[39m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1519\u001b[0m     \u001b[39m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1522\u001b[0m     \u001b[39m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m     \u001b[39m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Не удается найти указанный файл",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m dot \u001b[39m=\u001b[39m make_dot(output, params\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(model\u001b[39m.\u001b[39mnamed_parameters()))\n\u001b[0;32m     39\u001b[0m dot\u001b[39m.\u001b[39mformat \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpng\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 40\u001b[0m dot\u001b[39m.\u001b[39;49mrender(\u001b[39m\"\u001b[39;49m\u001b[39mneural_network_architecture\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m!r}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[39mfor\u001b[39;00m name, value \u001b[39min\u001b[39;00m deprecated\u001b[39m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe signature of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m will be reduced\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00msupported_number\u001b[39m}\u001b[39;00m\u001b[39m positional args\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(supported)\u001b[39m}\u001b[39;00m\u001b[39m: pass \u001b[39m\u001b[39m{\u001b[39;00mwanted\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39m as keyword arg(s)\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[39m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\graphviz\\rendering.py:122\u001b[0m, in \u001b[0;36mRender.render\u001b[1;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[0;32m    118\u001b[0m filepath \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave(filename, directory\u001b[39m=\u001b[39mdirectory, skip_existing\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    120\u001b[0m args\u001b[39m.\u001b[39mappend(filepath)\n\u001b[1;32m--> 122\u001b[0m rendered \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    124\u001b[0m \u001b[39mif\u001b[39;00m cleanup:\n\u001b[0;32m    125\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mdelete \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m, filepath)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m!r}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[39mfor\u001b[39;00m name, value \u001b[39min\u001b[39;00m deprecated\u001b[39m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe signature of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m will be reduced\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00msupported_number\u001b[39m}\u001b[39;00m\u001b[39m positional args\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(supported)\u001b[39m}\u001b[39;00m\u001b[39m: pass \u001b[39m\u001b[39m{\u001b[39;00mwanted\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39m as keyword arg(s)\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[39m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\graphviz\\backend\\rendering.py:324\u001b[0m, in \u001b[0;36mrender\u001b[1;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mFileExistsError(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39moutput file exists: \u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mfspath(outfile)\u001b[39m!r}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    322\u001b[0m cmd \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args\n\u001b[1;32m--> 324\u001b[0m execute\u001b[39m.\u001b[39;49mrun_check(cmd,\n\u001b[0;32m    325\u001b[0m                   cwd\u001b[39m=\u001b[39;49mfilepath\u001b[39m.\u001b[39;49mparent \u001b[39mif\u001b[39;49;00m filepath\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49mparts \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    326\u001b[0m                   quiet\u001b[39m=\u001b[39;49mquiet,\n\u001b[0;32m    327\u001b[0m                   capture_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    329\u001b[0m \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39mfspath(outfile)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\graphviz\\backend\\execute.py:84\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merrno \u001b[39m==\u001b[39m errno\u001b[39m.\u001b[39mENOENT:\n\u001b[1;32m---> 84\u001b[0m         \u001b[39mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m quiet \u001b[39mand\u001b[39;00m proc\u001b[39m.\u001b[39mstderr:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Определение модели нейронной сети\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Создание экземпляра модели\n",
    "model = Net()\n",
    "\n",
    "# Создание фиктивных входных данных\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# Выполнение прямого прохода по модели\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Создание графа\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.format = 'png'\n",
    "dot.render(\"neural_network_architecture\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
