{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Batch [0/1207], Loss: 0.6931\n",
      "Epoch [1/20], Batch [100/1207], Loss: 0.7462\n",
      "Epoch [1/20], Batch [200/1207], Loss: 0.7047\n",
      "Epoch [1/20], Batch [300/1207], Loss: 0.6770\n",
      "Epoch [1/20], Batch [400/1207], Loss: 0.6860\n",
      "Epoch [1/20], Batch [500/1207], Loss: 0.6915\n",
      "Epoch [1/20], Batch [600/1207], Loss: 0.7017\n",
      "Epoch [1/20], Batch [700/1207], Loss: 0.6878\n",
      "Epoch [1/20], Batch [800/1207], Loss: 0.6774\n",
      "Epoch [1/20], Batch [900/1207], Loss: 0.7058\n",
      "Epoch [1/20], Batch [1000/1207], Loss: 0.6035\n",
      "Epoch [1/20], Batch [1100/1207], Loss: 0.6914\n",
      "Epoch [1/20], Batch [1200/1207], Loss: 0.6592\n",
      "Test Accuracy: 61.28%\n",
      "Epoch [2/20], Batch [0/1207], Loss: 0.6013\n",
      "Epoch [2/20], Batch [100/1207], Loss: 0.6977\n",
      "Epoch [2/20], Batch [200/1207], Loss: 0.5696\n",
      "Epoch [2/20], Batch [300/1207], Loss: 0.7387\n",
      "Epoch [2/20], Batch [400/1207], Loss: 0.7012\n",
      "Epoch [2/20], Batch [500/1207], Loss: 0.7323\n",
      "Epoch [2/20], Batch [600/1207], Loss: 0.6325\n",
      "Epoch [2/20], Batch [700/1207], Loss: 0.6985\n",
      "Epoch [2/20], Batch [800/1207], Loss: 0.6609\n",
      "Epoch [2/20], Batch [900/1207], Loss: 0.5912\n",
      "Epoch [2/20], Batch [1000/1207], Loss: 0.6603\n",
      "Epoch [2/20], Batch [1100/1207], Loss: 0.6753\n",
      "Epoch [2/20], Batch [1200/1207], Loss: 0.6806\n",
      "Test Accuracy: 61.89%\n",
      "Epoch [3/20], Batch [0/1207], Loss: 0.6674\n",
      "Epoch [3/20], Batch [100/1207], Loss: 0.7367\n",
      "Epoch [3/20], Batch [200/1207], Loss: 0.5976\n",
      "Epoch [3/20], Batch [300/1207], Loss: 0.6250\n",
      "Epoch [3/20], Batch [400/1207], Loss: 0.6825\n",
      "Epoch [3/20], Batch [500/1207], Loss: 0.5723\n",
      "Epoch [3/20], Batch [600/1207], Loss: 0.6556\n",
      "Epoch [3/20], Batch [700/1207], Loss: 0.5765\n",
      "Epoch [3/20], Batch [800/1207], Loss: 0.6676\n",
      "Epoch [3/20], Batch [900/1207], Loss: 0.6693\n",
      "Epoch [3/20], Batch [1000/1207], Loss: 0.6920\n",
      "Epoch [3/20], Batch [1100/1207], Loss: 0.7299\n",
      "Epoch [3/20], Batch [1200/1207], Loss: 0.6880\n",
      "Test Accuracy: 62.07%\n",
      "Epoch [4/20], Batch [0/1207], Loss: 0.6657\n",
      "Epoch [4/20], Batch [100/1207], Loss: 0.6394\n",
      "Epoch [4/20], Batch [200/1207], Loss: 0.6262\n",
      "Epoch [4/20], Batch [300/1207], Loss: 0.6776\n",
      "Epoch [4/20], Batch [400/1207], Loss: 0.6554\n",
      "Epoch [4/20], Batch [500/1207], Loss: 0.7074\n",
      "Epoch [4/20], Batch [600/1207], Loss: 0.6502\n",
      "Epoch [4/20], Batch [700/1207], Loss: 0.6858\n",
      "Epoch [4/20], Batch [800/1207], Loss: 0.6860\n",
      "Epoch [4/20], Batch [900/1207], Loss: 0.6735\n",
      "Epoch [4/20], Batch [1000/1207], Loss: 0.6487\n",
      "Epoch [4/20], Batch [1100/1207], Loss: 0.6851\n",
      "Epoch [4/20], Batch [1200/1207], Loss: 0.6850\n",
      "Test Accuracy: 63.78%\n",
      "Epoch [5/20], Batch [0/1207], Loss: 0.6137\n",
      "Epoch [5/20], Batch [100/1207], Loss: 0.6415\n",
      "Epoch [5/20], Batch [200/1207], Loss: 0.6531\n",
      "Epoch [5/20], Batch [300/1207], Loss: 0.7291\n",
      "Epoch [5/20], Batch [400/1207], Loss: 0.6962\n",
      "Epoch [5/20], Batch [500/1207], Loss: 0.6586\n",
      "Epoch [5/20], Batch [600/1207], Loss: 0.6379\n",
      "Epoch [5/20], Batch [700/1207], Loss: 0.6240\n",
      "Epoch [5/20], Batch [800/1207], Loss: 0.6674\n",
      "Epoch [5/20], Batch [900/1207], Loss: 0.6966\n",
      "Epoch [5/20], Batch [1000/1207], Loss: 0.5927\n",
      "Epoch [5/20], Batch [1100/1207], Loss: 0.5969\n",
      "Epoch [5/20], Batch [1200/1207], Loss: 0.6700\n",
      "Test Accuracy: 62.55%\n",
      "Epoch [6/20], Batch [0/1207], Loss: 0.6436\n",
      "Epoch [6/20], Batch [100/1207], Loss: 0.6530\n",
      "Epoch [6/20], Batch [200/1207], Loss: 0.7226\n",
      "Epoch [6/20], Batch [300/1207], Loss: 0.6301\n",
      "Epoch [6/20], Batch [400/1207], Loss: 0.6566\n",
      "Epoch [6/20], Batch [500/1207], Loss: 0.6771\n",
      "Epoch [6/20], Batch [600/1207], Loss: 0.6204\n",
      "Epoch [6/20], Batch [700/1207], Loss: 0.5986\n",
      "Epoch [6/20], Batch [800/1207], Loss: 0.6562\n",
      "Epoch [6/20], Batch [900/1207], Loss: 0.6242\n",
      "Epoch [6/20], Batch [1000/1207], Loss: 0.6222\n",
      "Epoch [6/20], Batch [1100/1207], Loss: 0.6359\n",
      "Epoch [6/20], Batch [1200/1207], Loss: 0.6714\n",
      "Test Accuracy: 64.08%\n",
      "Epoch [7/20], Batch [0/1207], Loss: 0.6465\n",
      "Epoch [7/20], Batch [100/1207], Loss: 0.6911\n",
      "Epoch [7/20], Batch [200/1207], Loss: 0.6470\n",
      "Epoch [7/20], Batch [300/1207], Loss: 0.7030\n",
      "Epoch [7/20], Batch [400/1207], Loss: 0.6902\n",
      "Epoch [7/20], Batch [500/1207], Loss: 0.5988\n",
      "Epoch [7/20], Batch [600/1207], Loss: 0.6456\n",
      "Epoch [7/20], Batch [700/1207], Loss: 0.6764\n",
      "Epoch [7/20], Batch [800/1207], Loss: 0.6926\n",
      "Epoch [7/20], Batch [900/1207], Loss: 0.7119\n",
      "Epoch [7/20], Batch [1000/1207], Loss: 0.7502\n",
      "Epoch [7/20], Batch [1100/1207], Loss: 0.6492\n",
      "Epoch [7/20], Batch [1200/1207], Loss: 0.5986\n",
      "Test Accuracy: 64.47%\n",
      "Epoch [8/20], Batch [0/1207], Loss: 0.6648\n",
      "Epoch [8/20], Batch [100/1207], Loss: 0.6758\n",
      "Epoch [8/20], Batch [200/1207], Loss: 0.6303\n",
      "Epoch [8/20], Batch [300/1207], Loss: 0.6452\n",
      "Epoch [8/20], Batch [400/1207], Loss: 0.6338\n",
      "Epoch [8/20], Batch [500/1207], Loss: 0.7428\n",
      "Epoch [8/20], Batch [600/1207], Loss: 0.6707\n",
      "Epoch [8/20], Batch [700/1207], Loss: 0.6565\n",
      "Epoch [8/20], Batch [800/1207], Loss: 0.6745\n",
      "Epoch [8/20], Batch [900/1207], Loss: 0.6048\n",
      "Epoch [8/20], Batch [1000/1207], Loss: 0.7803\n",
      "Epoch [8/20], Batch [1100/1207], Loss: 0.6686\n",
      "Epoch [8/20], Batch [1200/1207], Loss: 0.6662\n",
      "Test Accuracy: 65.00%\n",
      "Epoch [9/20], Batch [0/1207], Loss: 0.6416\n",
      "Epoch [9/20], Batch [100/1207], Loss: 0.6595\n",
      "Epoch [9/20], Batch [200/1207], Loss: 0.6989\n",
      "Epoch [9/20], Batch [300/1207], Loss: 0.6489\n",
      "Epoch [9/20], Batch [400/1207], Loss: 0.5902\n",
      "Epoch [9/20], Batch [500/1207], Loss: 0.5736\n",
      "Epoch [9/20], Batch [600/1207], Loss: 0.6718\n",
      "Epoch [9/20], Batch [700/1207], Loss: 0.6014\n",
      "Epoch [9/20], Batch [800/1207], Loss: 0.6565\n",
      "Epoch [9/20], Batch [900/1207], Loss: 0.6054\n",
      "Epoch [9/20], Batch [1000/1207], Loss: 0.6720\n",
      "Epoch [9/20], Batch [1100/1207], Loss: 0.7439\n",
      "Epoch [9/20], Batch [1200/1207], Loss: 0.6784\n",
      "Test Accuracy: 64.20%\n",
      "Epoch [10/20], Batch [0/1207], Loss: 0.7205\n",
      "Epoch [10/20], Batch [100/1207], Loss: 0.5915\n",
      "Epoch [10/20], Batch [200/1207], Loss: 0.6872\n",
      "Epoch [10/20], Batch [300/1207], Loss: 0.6341\n",
      "Epoch [10/20], Batch [400/1207], Loss: 0.6148\n",
      "Epoch [10/20], Batch [500/1207], Loss: 0.5393\n",
      "Epoch [10/20], Batch [600/1207], Loss: 0.6200\n",
      "Epoch [10/20], Batch [700/1207], Loss: 0.5726\n",
      "Epoch [10/20], Batch [800/1207], Loss: 0.6722\n",
      "Epoch [10/20], Batch [900/1207], Loss: 0.6803\n",
      "Epoch [10/20], Batch [1000/1207], Loss: 0.6348\n",
      "Epoch [10/20], Batch [1100/1207], Loss: 0.6086\n",
      "Epoch [10/20], Batch [1200/1207], Loss: 0.5961\n",
      "Test Accuracy: 64.96%\n",
      "Epoch [11/20], Batch [0/1207], Loss: 0.5588\n",
      "Epoch [11/20], Batch [100/1207], Loss: 0.6298\n",
      "Epoch [11/20], Batch [200/1207], Loss: 0.6275\n",
      "Epoch [11/20], Batch [300/1207], Loss: 0.5840\n",
      "Epoch [11/20], Batch [400/1207], Loss: 0.6212\n",
      "Epoch [11/20], Batch [500/1207], Loss: 0.6339\n",
      "Epoch [11/20], Batch [600/1207], Loss: 0.6431\n",
      "Epoch [11/20], Batch [700/1207], Loss: 0.7193\n",
      "Epoch [11/20], Batch [800/1207], Loss: 0.6806\n",
      "Epoch [11/20], Batch [900/1207], Loss: 0.6577\n",
      "Epoch [11/20], Batch [1000/1207], Loss: 0.6421\n",
      "Epoch [11/20], Batch [1100/1207], Loss: 0.6876\n",
      "Epoch [11/20], Batch [1200/1207], Loss: 0.6660\n",
      "Test Accuracy: 64.45%\n",
      "Epoch [12/20], Batch [0/1207], Loss: 0.5965\n",
      "Epoch [12/20], Batch [100/1207], Loss: 0.5264\n",
      "Epoch [12/20], Batch [200/1207], Loss: 0.5868\n",
      "Epoch [12/20], Batch [300/1207], Loss: 0.6770\n",
      "Epoch [12/20], Batch [400/1207], Loss: 0.6771\n",
      "Epoch [12/20], Batch [500/1207], Loss: 0.5288\n",
      "Epoch [12/20], Batch [600/1207], Loss: 0.5763\n",
      "Epoch [12/20], Batch [700/1207], Loss: 0.6439\n",
      "Epoch [12/20], Batch [800/1207], Loss: 0.7177\n",
      "Epoch [12/20], Batch [900/1207], Loss: 0.6010\n",
      "Epoch [12/20], Batch [1000/1207], Loss: 0.5957\n",
      "Epoch [12/20], Batch [1100/1207], Loss: 0.5815\n",
      "Epoch [12/20], Batch [1200/1207], Loss: 0.5565\n",
      "Test Accuracy: 64.58%\n",
      "Epoch [13/20], Batch [0/1207], Loss: 0.6065\n",
      "Epoch [13/20], Batch [100/1207], Loss: 0.5547\n",
      "Epoch [13/20], Batch [200/1207], Loss: 0.6410\n",
      "Epoch [13/20], Batch [300/1207], Loss: 0.6027\n",
      "Epoch [13/20], Batch [400/1207], Loss: 0.6495\n",
      "Epoch [13/20], Batch [500/1207], Loss: 0.6205\n",
      "Epoch [13/20], Batch [600/1207], Loss: 0.6482\n",
      "Epoch [13/20], Batch [700/1207], Loss: 0.6955\n",
      "Epoch [13/20], Batch [800/1207], Loss: 0.6324\n",
      "Epoch [13/20], Batch [900/1207], Loss: 0.5752\n",
      "Epoch [13/20], Batch [1000/1207], Loss: 0.6458\n",
      "Epoch [13/20], Batch [1100/1207], Loss: 0.7218\n",
      "Epoch [13/20], Batch [1200/1207], Loss: 0.6323\n",
      "Test Accuracy: 65.38%\n",
      "Epoch [14/20], Batch [0/1207], Loss: 0.6064\n",
      "Epoch [14/20], Batch [100/1207], Loss: 0.5259\n",
      "Epoch [14/20], Batch [200/1207], Loss: 0.6666\n",
      "Epoch [14/20], Batch [300/1207], Loss: 0.6843\n",
      "Epoch [14/20], Batch [400/1207], Loss: 0.5921\n",
      "Epoch [14/20], Batch [500/1207], Loss: 0.7233\n",
      "Epoch [14/20], Batch [600/1207], Loss: 0.6979\n",
      "Epoch [14/20], Batch [700/1207], Loss: 0.6139\n",
      "Epoch [14/20], Batch [800/1207], Loss: 0.5357\n",
      "Epoch [14/20], Batch [900/1207], Loss: 0.5435\n",
      "Epoch [14/20], Batch [1000/1207], Loss: 0.5562\n",
      "Epoch [14/20], Batch [1100/1207], Loss: 0.6135\n",
      "Epoch [14/20], Batch [1200/1207], Loss: 0.5946\n",
      "Test Accuracy: 65.39%\n",
      "Epoch [15/20], Batch [0/1207], Loss: 0.5502\n",
      "Epoch [15/20], Batch [100/1207], Loss: 0.7080\n",
      "Epoch [15/20], Batch [200/1207], Loss: 0.4956\n",
      "Epoch [15/20], Batch [300/1207], Loss: 0.6037\n",
      "Epoch [15/20], Batch [400/1207], Loss: 0.6749\n",
      "Epoch [15/20], Batch [500/1207], Loss: 0.6402\n",
      "Epoch [15/20], Batch [600/1207], Loss: 0.5989\n",
      "Epoch [15/20], Batch [700/1207], Loss: 0.4785\n",
      "Epoch [15/20], Batch [800/1207], Loss: 0.6260\n",
      "Epoch [15/20], Batch [900/1207], Loss: 0.5728\n",
      "Epoch [15/20], Batch [1000/1207], Loss: 0.7064\n",
      "Epoch [15/20], Batch [1100/1207], Loss: 0.5787\n",
      "Epoch [15/20], Batch [1200/1207], Loss: 0.5831\n",
      "Test Accuracy: 64.39%\n",
      "Epoch [16/20], Batch [0/1207], Loss: 0.5577\n",
      "Epoch [16/20], Batch [100/1207], Loss: 0.5196\n",
      "Epoch [16/20], Batch [200/1207], Loss: 0.5803\n",
      "Epoch [16/20], Batch [300/1207], Loss: 0.6354\n",
      "Epoch [16/20], Batch [400/1207], Loss: 0.6870\n",
      "Epoch [16/20], Batch [500/1207], Loss: 0.6711\n",
      "Epoch [16/20], Batch [600/1207], Loss: 0.6158\n",
      "Epoch [16/20], Batch [700/1207], Loss: 0.7272\n",
      "Epoch [16/20], Batch [800/1207], Loss: 0.6638\n",
      "Epoch [16/20], Batch [900/1207], Loss: 0.6949\n",
      "Epoch [16/20], Batch [1000/1207], Loss: 0.5349\n",
      "Epoch [16/20], Batch [1100/1207], Loss: 0.5706\n",
      "Epoch [16/20], Batch [1200/1207], Loss: 0.5508\n",
      "Test Accuracy: 65.92%\n",
      "Epoch [17/20], Batch [0/1207], Loss: 0.6033\n",
      "Epoch [17/20], Batch [100/1207], Loss: 0.6636\n",
      "Epoch [17/20], Batch [200/1207], Loss: 0.6002\n",
      "Epoch [17/20], Batch [300/1207], Loss: 0.5877\n",
      "Epoch [17/20], Batch [400/1207], Loss: 0.5961\n",
      "Epoch [17/20], Batch [500/1207], Loss: 0.6840\n",
      "Epoch [17/20], Batch [600/1207], Loss: 0.6250\n",
      "Epoch [17/20], Batch [700/1207], Loss: 0.4697\n",
      "Epoch [17/20], Batch [800/1207], Loss: 0.6078\n",
      "Epoch [17/20], Batch [900/1207], Loss: 0.5927\n",
      "Epoch [17/20], Batch [1000/1207], Loss: 0.6309\n",
      "Epoch [17/20], Batch [1100/1207], Loss: 0.5621\n",
      "Epoch [17/20], Batch [1200/1207], Loss: 0.5421\n",
      "Test Accuracy: 64.52%\n",
      "Epoch [18/20], Batch [0/1207], Loss: 0.6445\n",
      "Epoch [18/20], Batch [100/1207], Loss: 0.5963\n",
      "Epoch [18/20], Batch [200/1207], Loss: 0.5734\n",
      "Epoch [18/20], Batch [300/1207], Loss: 0.6651\n",
      "Epoch [18/20], Batch [400/1207], Loss: 0.5453\n",
      "Epoch [18/20], Batch [500/1207], Loss: 0.6208\n",
      "Epoch [18/20], Batch [600/1207], Loss: 0.6363\n",
      "Epoch [18/20], Batch [700/1207], Loss: 0.6447\n",
      "Epoch [18/20], Batch [800/1207], Loss: 0.7643\n",
      "Epoch [18/20], Batch [900/1207], Loss: 0.6848\n",
      "Epoch [18/20], Batch [1000/1207], Loss: 0.6381\n",
      "Epoch [18/20], Batch [1100/1207], Loss: 0.5514\n",
      "Epoch [18/20], Batch [1200/1207], Loss: 0.5269\n",
      "Test Accuracy: 65.01%\n",
      "Epoch [19/20], Batch [0/1207], Loss: 0.5412\n",
      "Epoch [19/20], Batch [100/1207], Loss: 0.7457\n",
      "Epoch [19/20], Batch [200/1207], Loss: 0.5297\n",
      "Epoch [19/20], Batch [300/1207], Loss: 0.5600\n",
      "Epoch [19/20], Batch [400/1207], Loss: 0.5209\n",
      "Epoch [19/20], Batch [500/1207], Loss: 0.6699\n",
      "Epoch [19/20], Batch [600/1207], Loss: 0.6232\n",
      "Epoch [19/20], Batch [700/1207], Loss: 0.7089\n",
      "Epoch [19/20], Batch [800/1207], Loss: 0.6406\n",
      "Epoch [19/20], Batch [900/1207], Loss: 0.6971\n",
      "Epoch [19/20], Batch [1000/1207], Loss: 0.5230\n",
      "Epoch [19/20], Batch [1100/1207], Loss: 0.6692\n",
      "Epoch [19/20], Batch [1200/1207], Loss: 0.6236\n",
      "Test Accuracy: 64.25%\n",
      "Epoch [20/20], Batch [0/1207], Loss: 0.5532\n",
      "Epoch [20/20], Batch [100/1207], Loss: 0.5680\n",
      "Epoch [20/20], Batch [200/1207], Loss: 0.5648\n",
      "Epoch [20/20], Batch [300/1207], Loss: 0.5377\n",
      "Epoch [20/20], Batch [400/1207], Loss: 0.6286\n",
      "Epoch [20/20], Batch [500/1207], Loss: 0.5386\n",
      "Epoch [20/20], Batch [600/1207], Loss: 0.6923\n",
      "Epoch [20/20], Batch [700/1207], Loss: 0.6316\n",
      "Epoch [20/20], Batch [800/1207], Loss: 0.6551\n",
      "Epoch [20/20], Batch [900/1207], Loss: 0.5881\n",
      "Epoch [20/20], Batch [1000/1207], Loss: 0.6336\n",
      "Epoch [20/20], Batch [1100/1207], Loss: 0.6719\n",
      "Epoch [20/20], Batch [1200/1207], Loss: 0.6343\n",
      "Test Accuracy: 64.53%\n",
      "Модель создана\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "# Define the dataset and data loader\n",
    "train_folder = 'D:/Курсовой проект/Diploma project (Дилом)/Data/Train'\n",
    "test_folder =  'D:/Курсовой проект/Diploma project (Дилом)/Data/Test'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "train_dataset = datasets.ImageFolder(train_folder, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_folder, transform=transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Instantiate the neural network and define the loss function and optimizer\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the neural network\n",
    "for epoch in range(20):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print training progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{20}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Evaluate the neural network on the test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, targets in test_loader:\n",
    "            scores = model(data)\n",
    "            _, predicted = torch.max(scores.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "torch.save(model.state_dict(), 'person_detection_model.pth')\n",
    "print('Модель создана')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
