{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the transforms\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "torch.save(model.state_dict(), 'human_detection_model.pth')\n",
    "print(f\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Define transformations for data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load COCO dataset\n",
    "train_dataset = torchvision.datasets.CocoDetection(root='D:/СOCO Dataset/train2017', annFile='D:/СOCO Dataset/annotations/instances_train2017.json', transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define model architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = torchvision.models.resnet18(pretrained=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1000, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model and loss function\n",
    "model = Net()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training progress\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Save trained model\n",
    "torch.save(model.state_dict(), 'human_detection_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x000001BD54A0A160>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\tqdm\\std.py\", line 1145, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\tqdm\\std.py\", line 1262, in close\n",
      "    def close(self):\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=103.29s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.utils' has no attribute 'collate_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m train_dataset \u001b[39m=\u001b[39m CocoDetection(root\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:/Users/lolol/OneDrive/Документы/СOCO Dataset/train2017\u001b[39m\u001b[39m'\u001b[39m, annFile\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:/Users/lolol/OneDrive/Документы/СOCO Dataset/annotations/instances_train2017.json\u001b[39m\u001b[39m'\u001b[39m, transform\u001b[39m=\u001b[39mtransform)\n\u001b[0;32m     23\u001b[0m \u001b[39m# val_dataset = CocoDetection(root='coco/val2017', annFile='coco/annotations/instances_val2017.json', transform=transform)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[39m# Define data loaders\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, collate_fn\u001b[39m=\u001b[39mutils\u001b[39m.\u001b[39;49mcollate_fn)\n\u001b[0;32m     27\u001b[0m \u001b[39m# val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=utils.collate_fn)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[39m# Define model\u001b[39;00m\n\u001b[0;32m     30\u001b[0m model \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mget_model(num_classes\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchvision.utils' has no attribute 'collate_fn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision.utils as utils\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load COCO dataset\n",
    "train_dataset = CocoDetection(root='C:/Users/lolol/OneDrive/Документы/СOCO Dataset/train2017', annFile='C:/Users/lolol/OneDrive/Документы/СOCO Dataset/annotations/instances_train2017.json', transform=transform)\n",
    "val_dataset = CocoDetection(root='coco/val2017', annFile='coco/annotations/instances_val2017.json', transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=utils.collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=utils.collate_fn)\n",
    "\n",
    "# Define model\n",
    "model = utils.get_model(num_classes=2)\n",
    "\n",
    "# Set model to device\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Train model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch\n",
    "    utils.train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    # Evaluate on validation set\n",
    "    utils.evaluate(model, val_loader, device=device)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'human_detection_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
