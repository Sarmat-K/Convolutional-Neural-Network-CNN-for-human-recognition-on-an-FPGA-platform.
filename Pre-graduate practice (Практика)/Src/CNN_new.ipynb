{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Load the COCO dataset\n",
    "dataset = CocoDetection(root='D:/COCO Dataset/train2017', annFile='D:/COCO Dataset/annotations/instances_train2017.json', transform=ToTensor())\n",
    "\n",
    "# Load a pre-trained model  \n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Replace the classifier with a new one that has 1 output channel (person or not person)\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Run the model on an image\n",
    "image = dataset[0][0].to(device)\n",
    "outputs = model([image])\n",
    "\n",
    "# Print the predicted boxes and labels for each person in the image\n",
    "for i in range(len(outputs)):\n",
    "    boxes = outputs[i]['boxes']\n",
    "    labels = outputs[i]['labels']\n",
    "    for j in range(len(boxes)):\n",
    "        if labels[j] == 1:\n",
    "            print(f'Person {j}: {boxes[j]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=17.22s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 0: tensor([535.4479,  10.9487, 638.7635, 280.7072])\n",
      "Person 1: tensor([506.5498, 112.3238, 640.0000, 356.4161])\n",
      "Person 2: tensor([499.1276,  94.1813, 629.7615, 230.0587])\n",
      "Person 3: tensor([450.2258,  16.1875, 636.0623, 325.9147])\n",
      "Person 4: tensor([298.5901, 357.8939, 430.1841, 468.0307])\n",
      "Person 5: tensor([223.8287,   5.6326, 334.6924,  84.3947])\n",
      "Person 6: tensor([523.8218, 330.4018, 533.4785, 345.7201])\n",
      "Person 7: tensor([322.6916, 115.0905, 640.0000, 346.2043])\n",
      "Person 8: tensor([549.6943, 403.2085, 571.6334, 421.1720])\n",
      "Person 9: tensor([562.5801, 197.3125, 640.0000, 452.9887])\n",
      "Person 10: tensor([589.6063,  57.2560, 640.0000, 188.0374])\n",
      "Person 11: tensor([348.1002,  34.4026, 622.9925, 251.0898])\n",
      "Person 12: tensor([295.2576,  12.1948, 314.0999,  31.0676])\n",
      "Person 13: tensor([312.6689, 257.5293, 553.2711, 424.9976])\n",
      "Person 14: tensor([391.9442,   0.0000, 456.8119,  49.2766])\n",
      "Person 15: tensor([431.5002, 179.6529, 577.5114, 432.0287])\n",
      "Person 16: tensor([328.2457, 319.1385, 395.4215, 445.1360])\n",
      "Person 17: tensor([429.2135,  60.8186, 467.1483, 132.0053])\n",
      "Person 18: tensor([484.2056,  38.2747, 522.6899,  79.5952])\n",
      "Person 19: tensor([318.3348, 187.1715, 576.1122, 282.8214])\n",
      "Person 20: tensor([448.0511, 207.4594, 640.0000, 334.7076])\n",
      "Person 21: tensor([555.5363, 396.6420, 569.4682, 418.5358])\n",
      "Person 22: tensor([349.8660, 124.5558, 620.7236, 232.6826])\n",
      "Person 23: tensor([176.2717, 151.2606, 244.6156, 227.6738])\n",
      "Person 24: tensor([197.7529, 179.7970, 236.5448, 231.3828])\n",
      "Person 25: tensor([190.6507, 320.4906, 345.5135, 464.4941])\n",
      "Person 26: tensor([258.1506,  46.4173, 335.1739,  72.6459])\n",
      "Person 27: tensor([180.0062, 179.6307, 215.2387, 217.2169])\n",
      "Person 28: tensor([160.3342, 170.1607, 205.0427, 208.9672])\n",
      "Person 29: tensor([170.1540, 130.8560, 245.4448, 194.2106])\n",
      "Person 30: tensor([289.2131,   6.1749, 314.2549,  24.9811])\n",
      "Person 31: tensor([351.8622, 273.9783, 367.9879, 287.6546])\n",
      "Person 32: tensor([515.6852, 215.6878, 640.0000, 293.3517])\n",
      "Person 33: tensor([242.0425,   9.4707, 319.2467,  35.0478])\n",
      "Person 34: tensor([ 89.0028, 256.4891, 377.5644, 463.3637])\n",
      "Person 35: tensor([196.8843, 182.1726, 211.2310, 207.6457])\n",
      "Person 36: tensor([354.7647, 407.4866, 430.5489, 472.3381])\n",
      "Person 37: tensor([453.5671,  33.9980, 536.0707,  87.0635])\n",
      "Person 38: tensor([286.6134, 147.8724, 640.0000, 261.8719])\n",
      "Person 39: tensor([333.8525, 449.7219, 414.3579, 474.1211])\n",
      "Person 40: tensor([182.8798,  23.7021, 263.9837,  51.9787])\n",
      "Person 41: tensor([422.8730,   4.6461, 458.7174,  44.2685])\n",
      "Person 42: tensor([193.7411, 186.5983, 205.9698, 208.6104])\n",
      "Person 43: tensor([502.3430,  42.4996, 525.1253,  67.7369])\n",
      "Person 44: tensor([557.3438, 423.3152, 640.0000, 473.8777])\n",
      "Person 45: tensor([ 55.0670,  17.0542, 384.4149, 108.9783])\n",
      "Person 46: tensor([523.5770, 317.0381, 532.5463, 339.1550])\n",
      "Person 47: tensor([518.4204,  56.8478, 572.4896, 101.3294])\n",
      "Person 48: tensor([295.1323,  44.0172, 573.5291, 455.1798])\n",
      "Person 49: tensor([442.4009, 280.9798, 593.4725, 423.1186])\n",
      "Person 50: tensor([  5.3274,  63.3442, 166.5552, 168.3142])\n",
      "Person 51: tensor([122.5766, 107.4350, 159.1702, 143.8292])\n",
      "Person 52: tensor([396.5921,  34.9609, 445.4027,  78.9170])\n",
      "Person 53: tensor([198.1747, 189.7831, 212.8421, 217.2026])\n",
      "Person 54: tensor([132.0032, 117.7207, 147.7314, 150.6891])\n",
      "Person 55: tensor([576.1774, 112.6592, 640.0000, 189.6631])\n",
      "Person 56: tensor([128.4841,   0.4493, 406.1264,  76.2614])\n",
      "Person 57: tensor([188.5403, 188.1160, 209.1140, 212.5430])\n",
      "Person 58: tensor([229.6793, 339.8096, 335.0013, 443.0829])\n",
      "Person 59: tensor([551.3240, 360.6797, 565.4493, 394.0583])\n",
      "Person 60: tensor([504.9337, 406.2079, 640.0000, 470.8138])\n",
      "Person 61: tensor([194.3430, 197.9054, 211.3302, 219.7649])\n",
      "Person 62: tensor([395.6574, 374.3629, 640.0000, 472.4886])\n",
      "Person 63: tensor([  0.0000,  92.7507, 151.0350, 140.1122])\n",
      "Person 64: tensor([463.2343, 248.9296, 611.3464, 382.3732])\n",
      "Person 65: tensor([180.9411,   6.0087, 328.8257,  52.4793])\n",
      "Person 66: tensor([444.5484,  36.1597, 479.0822,  74.6663])\n",
      "Person 67: tensor([122.7184, 120.8128, 151.2281, 138.5983])\n",
      "Person 68: tensor([372.9335,  48.5302, 411.8964,  84.9022])\n",
      "Person 69: tensor([350.7741, 371.6162, 395.5945, 408.4055])\n",
      "Person 70: tensor([280.2908,  45.0536, 326.0859,  64.0531])\n",
      "Person 71: tensor([207.3767, 192.1526, 238.2241, 221.2381])\n",
      "Person 72: tensor([413.8417,  89.2409, 463.9426, 137.4916])\n",
      "Person 73: tensor([155.3049, 336.5497, 179.1279, 360.9622])\n",
      "Person 74: tensor([125.5416, 353.7224, 325.0827, 467.6456])\n",
      "Person 75: tensor([409.8041, 230.5755, 450.7099, 265.9729])\n",
      "Person 76: tensor([487.8603,  37.7245, 572.8774, 134.1934])\n",
      "Person 77: tensor([131.4738, 105.1073, 162.0223, 129.5424])\n",
      "Person 78: tensor([163.9639, 332.9287, 178.0814, 358.8492])\n",
      "Person 79: tensor([227.7012,   3.0784, 317.6447,  26.4053])\n",
      "Person 80: tensor([459.6867,  51.4229, 594.1641, 117.1503])\n",
      "Person 81: tensor([380.9160, 223.6441, 462.8407, 286.8926])\n",
      "Person 82: tensor([238.2383,   0.0000, 480.4518, 367.4244])\n",
      "Person 83: tensor([121.9062,  90.6183, 167.9789, 126.4982])\n",
      "Person 84: tensor([ 14.4609, 408.6716, 369.9969, 469.4957])\n",
      "Person 85: tensor([586.5823,   9.6177, 637.9918, 146.3594])\n",
      "Person 86: tensor([357.2669,   2.0944, 392.9327,  60.8572])\n",
      "Person 87: tensor([317.5956,   1.4865, 384.8412, 130.6980])\n",
      "Person 88: tensor([197.4616,  49.6519, 255.3492, 201.3155])\n",
      "Person 89: tensor([359.7228,   0.0000, 454.6948,  75.2739])\n",
      "Person 90: tensor([253.4450,  12.0059, 298.0211,  47.7056])\n",
      "Person 91: tensor([277.8080, 216.6127, 536.3307, 320.6400])\n",
      "Person 92: tensor([ 32.3507,  59.3176, 184.1264, 125.9385])\n",
      "Person 93: tensor([ 18.2707,  80.6452, 260.8436, 266.1414])\n",
      "Person 94: tensor([222.1922, 207.8292, 255.6974, 240.4196])\n",
      "Person 95: tensor([582.1843, 318.1734, 640.0000, 462.9629])\n",
      "Person 96: tensor([343.9618, 309.6403, 630.6852, 456.5635])\n",
      "Person 97: tensor([63.8926, 19.4260, 74.4158, 26.1907])\n",
      "Person 98: tensor([  1.6132, 352.9130,  31.2747, 388.2484])\n",
      "Person 99: tensor([511.7904, 324.0926, 639.8262, 457.9092])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor \n",
    "from torchvision.datasets import CocoDetection \n",
    "from torchvision.transforms import ToTensor \n",
    " \n",
    "# Load the COCO dataset \n",
    "dataset = CocoDetection(root='D:/COCO Dataset/train2017', annFile='D:/COCO Dataset/annotations/instances_train2017.json', transform=ToTensor()) \n",
    " \n",
    "# Load a pre-trained model   \n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) \n",
    " \n",
    "# Replace the classifier with a new one that has 1 output channel (person or not person) \n",
    "num_classes = 2  # 1 class (person) + background \n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features \n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    " \n",
    "# Move the model to the GPU if available \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "model.to(device) \n",
    " \n",
    "# Prepare the image for inference\n",
    "image, _ = dataset[0]\n",
    "image = image.unsqueeze(0) # Add batch dimension\n",
    "image = image.to(device)\n",
    " \n",
    "# Run the model on the image \n",
    "model.eval() # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(image)\n",
    " \n",
    "# Print the predicted boxes and labels for each person in the image \n",
    "for i in range(len(outputs)): \n",
    "    boxes = outputs[i]['boxes'] \n",
    "    labels = outputs[i]['labels'] \n",
    "    for j in range(len(boxes)): \n",
    "        if labels[j] == 1: \n",
    "            print(f'Person {j}: {boxes[j]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m     37\u001b[0m \u001b[39m# Main program loop to capture camera images and display the results \u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m cap \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mVideoCapture(\u001b[39m0\u001b[39;49m) \u001b[39m# 0 is for the default camera\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     ret, frame \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch \n",
    "import torchvision \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor \n",
    "from torchvision.transforms import ToTensor \n",
    " \n",
    "# Load a pre-trained model   \n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) \n",
    " \n",
    "# Replace the classifier with a new one that has 1 output channel (person or not person) \n",
    "num_classes = 2  # 1 class (person) + background \n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features \n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    " \n",
    "# Move the model to the GPU if available \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "model.to(device) \n",
    " \n",
    "# Function to detect people in the camera image \n",
    "def detect_people(image):\n",
    "    # Convert the image to a PyTorch tensor \n",
    "    image_t = ToTensor()(image).to(device)\n",
    "    \n",
    "    # Run the model on the image \n",
    "    model.eval() # Set to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model([image_t])\n",
    " \n",
    "    # Check if there are any people in the image \n",
    "    for i in range(len(outputs)): \n",
    "        labels = outputs[i]['labels'] \n",
    "        if 1 in labels:\n",
    "            return 1\n",
    " \n",
    "    return 0\n",
    " \n",
    "# Main program loop to capture camera images and display the results \n",
    "cap = cv2.VideoCapture(0) # 0 is for the default camera\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Detect people in the camera image \n",
    "    has_people = detect_people(frame)\n",
    "    \n",
    "    # Display the result on the image \n",
    "    if has_people:\n",
    "        cv2.putText(frame, 'Person detected', (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "    else:\n",
    "        cv2.putText(frame, 'No person detected', (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "    \n",
    "    # Display the image in a window \n",
    "    cv2.imshow('Camera', frame)\n",
    "    \n",
    "    # Exit if the user pressed 'q'\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    " \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
