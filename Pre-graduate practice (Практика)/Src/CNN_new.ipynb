{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Load the COCO dataset\n",
    "dataset = CocoDetection(root='D:/COCO Dataset/train2017', annFile='D:/COCO Dataset/annotations/instances_train2017.json', transform=ToTensor())\n",
    "\n",
    "# Load a pre-trained model  \n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Replace the classifier with a new one that has 1 output channel (person or not person)\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Run the model on an image\n",
    "image = dataset[0][0].to(device)\n",
    "outputs = model([image])\n",
    "\n",
    "# Print the predicted boxes and labels for each person in the image\n",
    "for i in range(len(outputs)):\n",
    "    boxes = outputs[i]['boxes']\n",
    "    labels = outputs[i]['labels']\n",
    "    for j in range(len(boxes)):\n",
    "        if labels[j] == 1:\n",
    "            print(f'Person {j}: {boxes[j]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Загрузка предобученной модели\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "\n",
    "# Сохранение модели на диск\n",
    "torch.save(model.state_dict(), 'R-CNN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=17.22s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 0: tensor([535.4479,  10.9487, 638.7635, 280.7072])\n",
      "Person 1: tensor([506.5498, 112.3238, 640.0000, 356.4161])\n",
      "Person 2: tensor([499.1276,  94.1813, 629.7615, 230.0587])\n",
      "Person 3: tensor([450.2258,  16.1875, 636.0623, 325.9147])\n",
      "Person 4: tensor([298.5901, 357.8939, 430.1841, 468.0307])\n",
      "Person 5: tensor([223.8287,   5.6326, 334.6924,  84.3947])\n",
      "Person 6: tensor([523.8218, 330.4018, 533.4785, 345.7201])\n",
      "Person 7: tensor([322.6916, 115.0905, 640.0000, 346.2043])\n",
      "Person 8: tensor([549.6943, 403.2085, 571.6334, 421.1720])\n",
      "Person 9: tensor([562.5801, 197.3125, 640.0000, 452.9887])\n",
      "Person 10: tensor([589.6063,  57.2560, 640.0000, 188.0374])\n",
      "Person 11: tensor([348.1002,  34.4026, 622.9925, 251.0898])\n",
      "Person 12: tensor([295.2576,  12.1948, 314.0999,  31.0676])\n",
      "Person 13: tensor([312.6689, 257.5293, 553.2711, 424.9976])\n",
      "Person 14: tensor([391.9442,   0.0000, 456.8119,  49.2766])\n",
      "Person 15: tensor([431.5002, 179.6529, 577.5114, 432.0287])\n",
      "Person 16: tensor([328.2457, 319.1385, 395.4215, 445.1360])\n",
      "Person 17: tensor([429.2135,  60.8186, 467.1483, 132.0053])\n",
      "Person 18: tensor([484.2056,  38.2747, 522.6899,  79.5952])\n",
      "Person 19: tensor([318.3348, 187.1715, 576.1122, 282.8214])\n",
      "Person 20: tensor([448.0511, 207.4594, 640.0000, 334.7076])\n",
      "Person 21: tensor([555.5363, 396.6420, 569.4682, 418.5358])\n",
      "Person 22: tensor([349.8660, 124.5558, 620.7236, 232.6826])\n",
      "Person 23: tensor([176.2717, 151.2606, 244.6156, 227.6738])\n",
      "Person 24: tensor([197.7529, 179.7970, 236.5448, 231.3828])\n",
      "Person 25: tensor([190.6507, 320.4906, 345.5135, 464.4941])\n",
      "Person 26: tensor([258.1506,  46.4173, 335.1739,  72.6459])\n",
      "Person 27: tensor([180.0062, 179.6307, 215.2387, 217.2169])\n",
      "Person 28: tensor([160.3342, 170.1607, 205.0427, 208.9672])\n",
      "Person 29: tensor([170.1540, 130.8560, 245.4448, 194.2106])\n",
      "Person 30: tensor([289.2131,   6.1749, 314.2549,  24.9811])\n",
      "Person 31: tensor([351.8622, 273.9783, 367.9879, 287.6546])\n",
      "Person 32: tensor([515.6852, 215.6878, 640.0000, 293.3517])\n",
      "Person 33: tensor([242.0425,   9.4707, 319.2467,  35.0478])\n",
      "Person 34: tensor([ 89.0028, 256.4891, 377.5644, 463.3637])\n",
      "Person 35: tensor([196.8843, 182.1726, 211.2310, 207.6457])\n",
      "Person 36: tensor([354.7647, 407.4866, 430.5489, 472.3381])\n",
      "Person 37: tensor([453.5671,  33.9980, 536.0707,  87.0635])\n",
      "Person 38: tensor([286.6134, 147.8724, 640.0000, 261.8719])\n",
      "Person 39: tensor([333.8525, 449.7219, 414.3579, 474.1211])\n",
      "Person 40: tensor([182.8798,  23.7021, 263.9837,  51.9787])\n",
      "Person 41: tensor([422.8730,   4.6461, 458.7174,  44.2685])\n",
      "Person 42: tensor([193.7411, 186.5983, 205.9698, 208.6104])\n",
      "Person 43: tensor([502.3430,  42.4996, 525.1253,  67.7369])\n",
      "Person 44: tensor([557.3438, 423.3152, 640.0000, 473.8777])\n",
      "Person 45: tensor([ 55.0670,  17.0542, 384.4149, 108.9783])\n",
      "Person 46: tensor([523.5770, 317.0381, 532.5463, 339.1550])\n",
      "Person 47: tensor([518.4204,  56.8478, 572.4896, 101.3294])\n",
      "Person 48: tensor([295.1323,  44.0172, 573.5291, 455.1798])\n",
      "Person 49: tensor([442.4009, 280.9798, 593.4725, 423.1186])\n",
      "Person 50: tensor([  5.3274,  63.3442, 166.5552, 168.3142])\n",
      "Person 51: tensor([122.5766, 107.4350, 159.1702, 143.8292])\n",
      "Person 52: tensor([396.5921,  34.9609, 445.4027,  78.9170])\n",
      "Person 53: tensor([198.1747, 189.7831, 212.8421, 217.2026])\n",
      "Person 54: tensor([132.0032, 117.7207, 147.7314, 150.6891])\n",
      "Person 55: tensor([576.1774, 112.6592, 640.0000, 189.6631])\n",
      "Person 56: tensor([128.4841,   0.4493, 406.1264,  76.2614])\n",
      "Person 57: tensor([188.5403, 188.1160, 209.1140, 212.5430])\n",
      "Person 58: tensor([229.6793, 339.8096, 335.0013, 443.0829])\n",
      "Person 59: tensor([551.3240, 360.6797, 565.4493, 394.0583])\n",
      "Person 60: tensor([504.9337, 406.2079, 640.0000, 470.8138])\n",
      "Person 61: tensor([194.3430, 197.9054, 211.3302, 219.7649])\n",
      "Person 62: tensor([395.6574, 374.3629, 640.0000, 472.4886])\n",
      "Person 63: tensor([  0.0000,  92.7507, 151.0350, 140.1122])\n",
      "Person 64: tensor([463.2343, 248.9296, 611.3464, 382.3732])\n",
      "Person 65: tensor([180.9411,   6.0087, 328.8257,  52.4793])\n",
      "Person 66: tensor([444.5484,  36.1597, 479.0822,  74.6663])\n",
      "Person 67: tensor([122.7184, 120.8128, 151.2281, 138.5983])\n",
      "Person 68: tensor([372.9335,  48.5302, 411.8964,  84.9022])\n",
      "Person 69: tensor([350.7741, 371.6162, 395.5945, 408.4055])\n",
      "Person 70: tensor([280.2908,  45.0536, 326.0859,  64.0531])\n",
      "Person 71: tensor([207.3767, 192.1526, 238.2241, 221.2381])\n",
      "Person 72: tensor([413.8417,  89.2409, 463.9426, 137.4916])\n",
      "Person 73: tensor([155.3049, 336.5497, 179.1279, 360.9622])\n",
      "Person 74: tensor([125.5416, 353.7224, 325.0827, 467.6456])\n",
      "Person 75: tensor([409.8041, 230.5755, 450.7099, 265.9729])\n",
      "Person 76: tensor([487.8603,  37.7245, 572.8774, 134.1934])\n",
      "Person 77: tensor([131.4738, 105.1073, 162.0223, 129.5424])\n",
      "Person 78: tensor([163.9639, 332.9287, 178.0814, 358.8492])\n",
      "Person 79: tensor([227.7012,   3.0784, 317.6447,  26.4053])\n",
      "Person 80: tensor([459.6867,  51.4229, 594.1641, 117.1503])\n",
      "Person 81: tensor([380.9160, 223.6441, 462.8407, 286.8926])\n",
      "Person 82: tensor([238.2383,   0.0000, 480.4518, 367.4244])\n",
      "Person 83: tensor([121.9062,  90.6183, 167.9789, 126.4982])\n",
      "Person 84: tensor([ 14.4609, 408.6716, 369.9969, 469.4957])\n",
      "Person 85: tensor([586.5823,   9.6177, 637.9918, 146.3594])\n",
      "Person 86: tensor([357.2669,   2.0944, 392.9327,  60.8572])\n",
      "Person 87: tensor([317.5956,   1.4865, 384.8412, 130.6980])\n",
      "Person 88: tensor([197.4616,  49.6519, 255.3492, 201.3155])\n",
      "Person 89: tensor([359.7228,   0.0000, 454.6948,  75.2739])\n",
      "Person 90: tensor([253.4450,  12.0059, 298.0211,  47.7056])\n",
      "Person 91: tensor([277.8080, 216.6127, 536.3307, 320.6400])\n",
      "Person 92: tensor([ 32.3507,  59.3176, 184.1264, 125.9385])\n",
      "Person 93: tensor([ 18.2707,  80.6452, 260.8436, 266.1414])\n",
      "Person 94: tensor([222.1922, 207.8292, 255.6974, 240.4196])\n",
      "Person 95: tensor([582.1843, 318.1734, 640.0000, 462.9629])\n",
      "Person 96: tensor([343.9618, 309.6403, 630.6852, 456.5635])\n",
      "Person 97: tensor([63.8926, 19.4260, 74.4158, 26.1907])\n",
      "Person 98: tensor([  1.6132, 352.9130,  31.2747, 388.2484])\n",
      "Person 99: tensor([511.7904, 324.0926, 639.8262, 457.9092])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor \n",
    "from torchvision.datasets import CocoDetection \n",
    "from torchvision.transforms import ToTensor \n",
    " \n",
    "# Load the COCO dataset \n",
    "dataset = CocoDetection(root='D:/COCO Dataset/train2017', annFile='D:/COCO Dataset/annotations/instances_train2017.json', transform=ToTensor()) \n",
    " \n",
    "# Load a pre-trained model   \n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) \n",
    " \n",
    "# Replace the classifier with a new one that has 1 output channel (person or not person) \n",
    "num_classes = 2  # 1 class (person) + background \n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features \n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    " \n",
    "# Move the model to the GPU if available \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "model.to(device) \n",
    " \n",
    "# Prepare the image for inference\n",
    "image, _ = dataset[0]\n",
    "image = image.unsqueeze(0) # Add batch dimension\n",
    "image = image.to(device)\n",
    " \n",
    "# Run the model on the image \n",
    "model.eval() # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(image)\n",
    " \n",
    "# Print the predicted boxes and labels for each person in the image \n",
    "for i in range(len(outputs)): \n",
    "    boxes = outputs[i]['boxes'] \n",
    "    labels = outputs[i]['labels'] \n",
    "    for j in range(len(boxes)): \n",
    "        if labels[j] == 1: \n",
    "            print(f'Person {j}: {boxes[j]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Загрузка и предобработка данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CocoDetection(root='./data', annFile='./annotations_trainval2014.json', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Определение архитектуры нейронной сети\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.fc1 = nn.Linear(32 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)  # Количество классов (person и background)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Обучение модели\n",
    "for epoch in range(10):  # Количество эпох\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Обучение завершено!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
